{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python AI: How to Build a Neural Network & Make Predictions\n",
    "\n",
    "Python is an excellent language for studying Artificial Intelligence. Deep Learning is a technique used to make predictions using data that heavily relies on neural networks. The following is how we code a neural network. \n",
    "\n",
    "In a production environment you would use a deep learning framework like TensorFlow, or PyTorch instead of building your own neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal of Artificial Intelligence is to make a computer 'think' or solve problems that require thinking. Machine learning (ML) and deep learning (DL) are also approaches to solving problems. The difference between these techniques and a Python script is that ML and DL use training data instead of hard-coded rules, but all of them can be used to solve problems using AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning: is a technique in which you train the system to solve a problem instead of explicitly programming the rules. Using Statistical models we can program models which can approximate the behaviour of a phenomena. A common machine learning task is supervised learning, in which you have a dataset with inputs and known outputs. The task is to use this dataset to train a model that predicts the correct outputs based on the inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Common Machine Learning Task is 'Supervised Learning' in which you have a dataset with inputs and known outputs. The task is to use this dataset to train a model that predicts the correct outputs based on the inputs. The combination of training data with the machine learning algorithm creates the model. \n",
    "\n",
    "The Goal of supervised learning tasks is to make predictions for new, unseen data. To do that you assume this data follows a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering: Another name for input data is 'feature' and 'feature engineering' is the process of extracting features from raw data. When dealing with different kinds of data, you need to figure out ways to represent this data in order to extract meaningful information from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning: a technique in which you let the neural network figure out by itself which features are important instead of applying feature engineering techniques. This means that with deep learning, you can bypass the feature engineering process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Main Concepts: \n",
    "1. Taking the input data\n",
    "2. Making a prediction\n",
    "3. Comparing the prediction to the desired output\n",
    "4. Adjusting its internal state to predict correctly next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors, layers, and linear regression are some of the building blocks of neural networks. The data is stored as vectors, and with Python you store these vectors in arrays. Each layer transforms the data that comes from the previous layer. You can think of each layer as a feature engineering step, because each layer extracts some representation of the data that came previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear regression Model: Used when you eed to estimate the relationship between a dependent variable and two or more independent variables. Linear Regression is a method applied when you approximate the relationship between the variables as linear. By modeling the relationship between the variables as linear, you can express the dependent variable as a weighted sum of the independent variables. So Each input will be multiplied by a vector called weight. Besides the weights and the independent variables you also add antoher vector: the bias. It sets the result when all other independent variables are equal to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping the Inputs of the Neural Network With NumPy: You’ll use NumPy to represent the input vectors of the network as arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input this into your shell\n",
    "$ python -m venv ~/.my-env\n",
    "$ source ~/.my-env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(my-env) $ python -m pip install ipython numpy matplotlib\n",
    "(my-env) $ ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’re ready to start coding. This is the code for computing the dot product of input_vector and weights_1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: input_vector = [1.72, 1.23]\n",
    "In [2]: weights_1 = [1.26, 0]\n",
    "In [3]: weights_2 = [2.17, 0.32]\n",
    "\n",
    "In [4]: # Computing the dot product of input_vector and weights_1\n",
    "In [5]: first_indexes_mult = input_vector[0] * weights_1[0]\n",
    "In [6]: second_indexes_mult = input_vector[1] * weights_1[1]\n",
    "In [7]: dot_product_1 = first_indexes_mult + second_indexes_mult\n",
    "\n",
    "In [8]: print(f\"The dot product is: {dot_product_1}\")\n",
    "Out[8]: The dot product is: 2.1672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of the dot product is 2.1672. Now that you know how to compute the dot product,\n",
    "# it’s time to use np.dot() from NumPy. Here’s how to compute dot_product_1 using np.dot():\n",
    "\n",
    "In [9]: import numpy as np\n",
    "\n",
    "In [10]: dot_product_1 = np.dot(input_vector, weights_1)\n",
    "\n",
    "In [11]: print(f\"The dot product is: {dot_product_1}\")\n",
    "Out[11]: The dot product is: 2.1672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot() does the same thing you did before, but now you just need to specify the two arrays as arguments.\n",
    "# Now let’s compute the dot product of input_vector and weights_2:\n",
    "\n",
    "In [10]: dot_product_2 = np.dot(input_vector, weights_2)\n",
    "\n",
    "In [11]: print(f\"The dot product is: {dot_product_2}\")\n",
    "Out[11]: The dot product is: 4.1259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you’ll train a model to make predictions that have only two possible outcomes. The output result can be either 0 or 1. This is a classification problem, a subset of supervised learning problems in which you have a dataset with the inputs and the known targets. These are the inputs and the outputs of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is the variable you want to predict. In this example, you’re dealing with a dataset that consists of numbers. This isn’t common in a real production scenario. Usually, when there’s a need for a deep learning model, the data is presented in files, such as images or text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Your First Prediction\n",
    "Since this is your very first neural network, you’ll keep things straightforward and build a network with only two layers. So far, you’ve seen that the only two operations used inside the neural network were the dot product and a sum. Both are linear operations.\n",
    "\n",
    "If you add more layers but keep using only linear operations, then adding more layers would have no effect because each layer will always have some correlation with the input of the previous layer. This implies that, for a network with multiple layers, there would always be a network with fewer layers that predicts the same results.\n",
    "\n",
    "What you want is to find an operation that makes the middle layers sometimes correlate with an input and sometimes not correlate.\n",
    "\n",
    "You can achieve this behavior by using nonlinear functions. These nonlinear functions are called activation functions. There are many types of activation functions. The ReLU (rectified linear unit), for example, is a function that converts all negative numbers to zero. This means that the network can “turn off” a weight if it’s negative, adding nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [12]: # Wrapping the vectors in NumPy arrays\n",
    "In [13]: input_vector = np.array([1.66, 1.56])\n",
    "In [14]: weights_1 = np.array([1.45, -0.66])\n",
    "In [15]: bias = np.array([0.0])\n",
    "\n",
    "In [16]: def sigmoid(x):\n",
    "   ...:     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "In [17]: def make_prediction(input_vector, weights, bias):\n",
    "   ...:      layer_1 = np.dot(input_vector, weights) + bias\n",
    "   ...:      layer_2 = sigmoid(layer_1)\n",
    "   ...:      return layer_2\n",
    "\n",
    "In [18]: prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "In [19]: print(f\"The prediction result is: {prediction}\")\n",
    "Out[19]: The prediction result is: [0.7985731]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw prediction result is 0.79, which is higher than 0.5, so the output is 1. The network made a correct prediction. Now try it with another input vector, np.array([2, 1.5]). The correct result for this input is 0. You’ll only need to change the input_vector variable since all the other parameters remain the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [20]: # Changing the value of input_vector\n",
    "In [21]: input_vector = np.array([2, 1.5])\n",
    "\n",
    "In [22]: prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "In [23]: print(f\"The prediction result is: {prediction}\")\n",
    "Out[23]: The prediction result is: [0.87101915]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Your First Neural Network\n",
    "In the process of training the neural network, you first assess the error and then adjust the weights accordingly. To adjust the weights, you’ll use the gradient descent and backpropagation algorithms. Gradient descent is applied to find the direction and the rate to update the parameters.\n",
    "\n",
    "Before making any changes in the network, you need to compute the error. That’s what you’ll do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the Prediction Error\n",
    "To understand the magnitude of the error, you need to choose a way to measure it. The function used to measure the error is called the cost function, or loss function. In this tutorial, you’ll use the mean squared error (MSE) as your cost function. You compute the MSE in two steps:\n",
    "\n",
    "Compute the difference between the prediction and the target.\n",
    "Multiply the result by itself.\n",
    "The network can make a mistake by outputting a value that’s higher or lower than the correct value. Since the MSE is the squared difference between the prediction and the correct result, with this metric you’ll always end up with a positive value.\n",
    "\n",
    "This is the complete expression to compute the error for the last previous prediction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [24]: target = 0\n",
    "\n",
    "In [25]: mse = np.square(prediction - target)\n",
    "\n",
    "In [26]: print(f\"Prediction: {prediction}; Error: {mse}\")\n",
    "Out[26]: Prediction: [0.87101915]; Error: [0.7586743596667225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding How to Reduce the Error\n",
    "The goal is to change the weights and bias variables so you can reduce the error. To understand how this works, you’ll change only the weights variable and leave the bias fixed for now. You can also get rid of the sigmoid function and use only the result of layer_1. All that’s left is to figure out how you can modify the weights so that the error goes down.\n",
    "\n",
    "You compute the MSE by doing error = np.square(prediction - target). If you treat (prediction - target) as a single variable x, then you have error = np.square(x), which is a quadratic function. Here’s how the function looks if you plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [27]: derivative = 2 * (prediction - target)\n",
    "\n",
    "In [28]: print(f\"The derivative is {derivative}\")\n",
    "Out[28]: The derivative is: [1.7420383]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 1.74, a positive number, so you need to decrease the weights. You do that by subtracting the derivative result of the weights vector. Now you can update weights_1 accordingly and predict again to see how it affects the prediction result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [29]: # Updating the weights\n",
    "In [30]: weights_1 = weights_1 - derivative\n",
    "\n",
    "In [31]: prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "In [32]: error = (prediction - target) ** 2\n",
    "\n",
    "In [33]: print(f\"Prediction: {prediction}; Error: {error}\")\n",
    "Out[33]: Prediction: [0.01496248]; Error: [0.00022388]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [36]: def sigmoid_deriv(x):\n",
    "   ...:     return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "In [37]: derror_dprediction = 2 * (prediction - target)\n",
    "In [38]: layer_1 = np.dot(input_vector, weights_1) + bias\n",
    "In [39]: dprediction_dlayer1 = sigmoid_deriv(layer_1)\n",
    "In [40]: dlayer1_dbias = 1\n",
    "\n",
    "In [41]: derror_dbias = (\n",
    "   ...:     derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "   ...: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.weights = np.array([np.random.randn(), np.random.randn()])\n",
    "        self.bias = np.random.randn()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_deriv(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "        return prediction\n",
    "\n",
    "    def _compute_gradients(self, input_vector, target):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "\n",
    "        derror_dprediction = 2 * (prediction - target)\n",
    "        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)\n",
    "        dlayer1_dbias = 1\n",
    "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
    "\n",
    "        derror_dbias = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "        )\n",
    "        derror_dweights = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    "        )\n",
    "\n",
    "        return derror_dbias, derror_dweights\n",
    "\n",
    "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
    "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
    "        self.weights = self.weights - (\n",
    "            derror_dweights * self.learning_rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [42]: learning_rate = 0.1\n",
    "\n",
    "In [43]: neural_network = NeuralNetwork(learning_rate)\n",
    "\n",
    "In [44]: neural_network.predict(input_vector)\n",
    "Out[44]: array([0.79412963])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # ...\n",
    "\n",
    "    def train(self, input_vectors, targets, iterations):\n",
    "        cumulative_errors = []\n",
    "        for current_iteration in range(iterations):\n",
    "            # Pick a data instance at random\n",
    "            random_data_index = np.random.randint(len(input_vectors))\n",
    "\n",
    "            input_vector = input_vectors[random_data_index]\n",
    "            target = targets[random_data_index]\n",
    "\n",
    "            # Compute the gradients and update the weights\n",
    "            derror_dbias, derror_dweights = self._compute_gradients(\n",
    "                input_vector, target\n",
    "            )\n",
    "\n",
    "            self._update_parameters(derror_dbias, derror_dweights)\n",
    "\n",
    "            # Measure the cumulative error for all the instances\n",
    "            if current_iteration % 100 == 0:\n",
    "                cumulative_error = 0\n",
    "                # Loop through all the instances to measure the error\n",
    "                for data_instance_index in range(len(input_vectors)):\n",
    "                    data_point = input_vectors[data_instance_index]\n",
    "                    target = targets[data_instance_index]\n",
    "\n",
    "                    prediction = self.predict(data_point)\n",
    "                    error = np.square(prediction - target)\n",
    "\n",
    "                    cumulative_error = cumulative_error + error\n",
    "                cumulative_errors.append(cumulative_error)\n",
    "\n",
    "        return cumulative_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [45]: # Paste the NeuralNetwork class code here\n",
    "   ...: # (and don't forget to add the train method to the class)\n",
    "\n",
    "In [46]: import matplotlib.pyplot as plt\n",
    "\n",
    "In [47]: input_vectors = np.array(\n",
    "   ...:     [\n",
    "   ...:         [3, 1.5],\n",
    "   ...:         [2, 1],\n",
    "   ...:         [4, 1.5],\n",
    "   ...:         [3, 4],\n",
    "   ...:         [3.5, 0.5],\n",
    "   ...:         [2, 0.5],\n",
    "   ...:         [5.5, 1],\n",
    "   ...:         [1, 1],\n",
    "   ...:     ]\n",
    "   ...: )\n",
    "\n",
    "In [48]: targets = np.array([0, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "In [49]: learning_rate = 0.1\n",
    "\n",
    "In [50]: neural_network = NeuralNetwork(learning_rate)\n",
    "\n",
    "In [51]: training_error = neural_network.train(input_vectors, targets, 10000)\n",
    "\n",
    "In [52]: plt.plot(training_error)\n",
    "In [53]: plt.xlabel(\"Iterations\")\n",
    "In [54]: plt.ylabel(\"Error for all training instances\")\n",
    "In [54]: plt.savefig(\"cumulative_error.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
